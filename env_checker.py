"""
- í™˜ê²½í™•ì¸ ë° ê¸°ë³¸ ì„¤ì •
"""

# í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ : ì„¤ì¹˜ ì™„ë£Œ
import subprocess 
import sys
import torch
import platform
import psutil
import os
from typing import Dict, Tuple, Optional

class EnvironmentChecker:
    '''ì‹œìŠ¤í…œ í™˜ê²½ì„ í™•ì¸í•˜ê³  vLLM ì„¤ì •ì— í•„ìš”í•œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í´ë˜ìŠ¤'''
    def __init__(self):
        self.system_info = {}
        self.gpu_info = {}
        self.requirements = {
            'python_version' : (3,8),
            'cuda_version' : 12.0,
            'min_vram' : 4, # GB
            'max_vram' : 8, # GB
        }
    def check_python_version(self) -> Tuple[bool, str]:
        '''python ë²„ì „ ì²´í¬'''
        current_version = sys.version_info
        min_version = self.requirements['python_version']
        
        version_str = f"{current_version.major}.{current_version.micro}"
        is_compatible = current_version >= min_version
        
        return is_compatible, version_str
    
    def check_cuda_availability(self) -> Tuple[bool, Optional[str]]:
        '''CUDA ì„¤ì¹˜ ë° Pytorch CUDA ì§€ì› í™•ì¸'''
        cuda_available = torch.cuda.is_available()
        if cuda_available:
            cuda_version = torch.version.cuda
            return True, cuda_version
        else:
            return False, None
    def check_gpu_memory(self) -> Tuple[bool, Dict]:
        """GPU  ë©”ëª¨ë¦¬ ì •ë³´ í™•ì¸"""
        if not torch.cuda.is_available():
            return False, {}
        
        gpu_info = {}
        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            total_memory = torch.cuda.get_device_properties(i).total_memory
            total_memory_gb = total_memory / (1024 ** 3)
            gpu_info[f"gpu_{i}"] = {
                'name': gpu_name,
                'total_memory_gb': round(total_memory_gb,2),
                'is_sufficient': total_memory_gb >= self.requirements['min_vram'],
            }
        return True, gpu_info
    
    def check_system_memory(self) -> Tuple[bool, float]:
        """ì‹œìŠ¤í…œ ram í™•ì¸"""
        total_ram_gb = psutil.virtual_memory().total / (1024 ** 3)
        is_sufficient = total_ram_gb >= self.requirements['min_vram']
        
        return is_sufficient, round(total_ram_gb,2)
    
    def check_disk_space(self,path:str ='.') -> Tuple[bool, float]:
        '''ë””ìŠ¤í¬ ê³µê°„ í™•ì¸(ëª¨ë¸ ë¡œì»¬ ì €ì¥ì„ ìœ„í•¨)'''
        disk_usage = psutil.disk_usage(path)
        fee_space = disk_usage.free / (1024 ** 3)
        # ìµœì†Œ 50gb ì´ìƒ ê¶Œì¥ (ëª¨ë¸ + ìºì‹œ)
        is_sufficient = fee_space >= 50
        
        return is_sufficient, round(fee_space,2)
    def get_system_info(self) -> Dict:
        '''ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘'''
        python_ok, python_version = self.check_python_version()
        cuda_ok, cuda_version = self.check_cuda_availability()
        gpu_ok, gpu_details  = self.check_gpu_memory()
        ram_ok, ram_size = self.check_system_memory()
        disk_ok, disk_space = self.check_disk_space()
        system_info = {
            'platform' : platform.system(),
            'platform_version' : platform.version(),
            'architecture' : platform.machine(),
            'python' : {
                'version' : python_version,
                'compatible' : python_ok,
            },
            'cuda' : {
                'available' : cuda_ok,
                'version' : cuda_version,
                'compatible' : cuda_ok and cuda_version is not None,
            },
            'gpu' : {
                'available' : gpu_ok,
                'details' : gpu_details,
            },
            'memory' : {
                'ram_gb' : ram_size,
                'ram_sufficient' : ram_ok,
                'disk_free_gb' : disk_space,
                'disk_sufficient' : disk_ok,
            }
        }
        return system_info
    
    def print_environment_report(self):
        """í™˜ê²½ ì •ë³´ ì¶œë ¥"""
        info = self.get_system_info()
        
        print("í™˜ê²½ í™•ì¸ ë³´ê³ ì„œ")
        print("=" * 50)
        
        # ê¸°ë³¸ ì‹œìŠ¤í…œ ì •ë³´
        print(f"ìš´ì˜ì²´ì œ: {info['platform']} ({info['architecture']})")
        print(f"Python: {info['python']['version']} {'ok' if info['python']['compatible'] else 'ERROR'}")
        
        # CUDA ì •ë³´
        cuda_status = "ok" if info['cuda']['compatible'] else "ERROR"
        print(f"**CUDA: {info['cuda']['version'] or 'Not Available'} {cuda_status}")
        
        # GPU ì •ë³´
        if info['gpu']['available']:
            print("***GPU ì •ë³´:")
            for gpu_id, gpu_data in info['gpu']['details'].items():
                status = "ok" if gpu_data['is_sufficient'] else "ERROR"
                print(f"   - {gpu_data['name']}: {gpu_data['total_memory_gb']}GB {status}")
        else:
            print("****GPU: ì‚¬ìš© ë¶ˆê°€ ERROR")
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        ram_status = "ok" if info['memory']['ram_sufficient'] else "ERROR"
        disk_status = "ok" if info['memory']['disk_sufficient'] else "ERROR"
        print(f"****RAM: {info['memory']['ram_gb']}GB {ram_status}")
        print(f"**ì—¬ìœ  ë””ìŠ¤í¬: {info['memory']['disk_free_gb']}GB {disk_status}")
        
        # ì „ì²´ í˜¸í™˜ì„± ì²´í¬
        print("\n**vLLM í˜¸í™˜ì„± ì²´í¬:")
        all_compatible = all([
            info['python']['compatible'],
            info['cuda']['compatible'],
            any(gpu['is_sufficient'] for gpu in info['gpu']['details'].values()) if info['gpu']['details'] else False,
            info['memory']['ram_sufficient'],
            info['memory']['disk_sufficient']
        ])
        
        if all_compatible:
            print("ëª¨ë“  ìš”êµ¬ì‚¬í•­ ë§Œì¡±, vLLM ì„¤ì¹˜ë¥¼ ì§„í–‰ ê°€ëŠ¥")
        else:
            print("ì¼ë¶€ ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì•„ë˜ ê¶Œì¥ì‚¬í•­ì„ í™•ì¸í•˜ì„¸ìš”:")
            if not info['python']['compatible']:
                print("   - Python 3.8 ì´ìƒ í•„ìš”")
            if not info['cuda']['compatible']:
                print("   - CUDA ì„¤ì¹˜ ë˜ëŠ” PyTorch CUDA ë²„ì „ í™•ì¸ í•„ìš”")
            if not any(gpu['is_sufficient'] for gpu in info['gpu']['details'].values()) if info['gpu']['details'] else True:
                print("   - ìµœì†Œ 4GB VRAMì˜ GPU í•„ìš”")
            if not info['memory']['ram_sufficient']:
                print("   - ìµœì†Œ 16GB RAM ê¶Œì¥")
            if not info['memory']['disk_sufficient']:
                print("   - ìµœì†Œ 50GB ì—¬ìœ  ë””ìŠ¤í¬ ê³µê°„ í•„ìš”")
                
def create_requirements_file(filename:str='requirements.txt'):
    """í”„ë¡œì íŠ¸ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª©ë¡ì„ requirements.txtë¡œ ìƒì„±"""
    requirements = [
        "# vLLM ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬(torchëŠ” cuda ë²„ì ¼ ì²´í¬í•˜ë©´ì„œ ì„¤ì¹˜ í•„ìš”)",
        "vllm>=0.2.7",
        "torch>=2.0.0",
        "transformers>=4.35.0",
        "tokenizers>=0.14.0",
        "",
        "# LangGraph ë° LangChain",
        "langgraph>=0.0.30",
        "langchain>=0.1.0",
        "langchain-community>=0.0.10",
        "",
        "# ìœ í‹¸ë¦¬í‹°",
        "requests>=2.31.0",
        "aiohttp>=3.9.0",
        "pydantic>=2.5.0",
        "python-dotenv>=1.0.0",
        "",
        "# ê°œë°œ ë° í…ŒìŠ¤íŠ¸",
        "pytest>=7.4.0",
        "pytest-asyncio>=0.21.0",
        "black>=23.0.0",
        "flake8>=6.1.0",
        "",
        "# ëª¨ë‹ˆí„°ë§",
        "psutil>=5.9.0",
        "nvidia-ml-py>=12.535.0"
    ]
    
    with open(filename, "w", encoding="utf-8") as f:
        f.write("\n".join(requirements))
    
    print("ğŸ“„ requirements.txt íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
if __name__ == "__main__":
    # í™˜ê²½ ì²´ì»¤ ì‹¤í–‰
    checker = EnvironmentChecker()
    checker.print_environment_report()
    
    # í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒì„±
    print('='*50)
    create_requirements_file('standard_requirements.txt')