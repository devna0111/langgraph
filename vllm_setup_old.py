import subprocess
import sys
import os
import json
import requests
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

@dataclass
class ModelConfig :
    ''' ëª¨ë¸ ì„¤ì • ì •ë³´ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤'''
    name : str
    size : str
    quantization : str
    memory_requirement : float # GB
    context_length : int
    download_url : Optional[str] = None
    
class VLLMSetup:
    ''' vLLMê³¼ Qwen2.5 ëª¨ë¸ ì„¤ì •ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤'''
    def __init__(self, base_dir : str = './models') :
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)
        
        # RTX 4060(í™˜ê²½ ì»´í“¨í„°) ìµœì í™” ëœ ëª¨ë¸ ì„¤ì •
        self.available_models = {
            "qwen2.5-7b-instruct-kowiki-q8_0" : ModelConfig(
                name = "qwen2.5-7b-instruct-kowiki-q8_0",
                size = "7B",
                quantization = "Q8_0", # INT8 ì–‘ìí™”
                memory_requirement = 7.5, # GB
                context_length = 32768, # ??
            ),
            "qwen2.5-7b-instruct-kowiki-q4_k_m":ModelConfig(
                name = "qwen2.5-7b-instruct-kowiki-q4_k_m",
                size = "7B",
                quantization = "Q4_K_M", # INT4 ì–‘ìí™” : ë©”ëª¨ë¦¬ ì ˆì•½
                memory_requirement = 4.5, # GB
                context_length = 32768, # ??
            )
        }
        
        self.selected_model = None
        self.vllm_server_process = None

    def check_ollama_installed(self) -> Tuple[bool, Optional[str]] :
        ''' ollama ì„¤ì¹˜ ì—¬ë¶€ë¥¼ í™•ì¸ '''
        try :
            result = subrocess.run(
                ['ollama', '--version'],
                capture_output=True,
                text=True,
                check=True
            )
            version = result.stdout.strip()
            return True, version
        except (subprocess.CalledProcessError, FileNotFoundError) :
            return False, None
    
    def install_ollama(self) -> bool :
        ''' ollama ì„¤ì¹˜ (Ubuntu, Linux í™˜ê²½) '''
        print("ollama ì„¤ì¹˜ ì‹œì‘")
        
        try :
            # Ollama ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
            install_cmd = "curl -fsSL https://ollama.com/install.sh | sh"
            result = subprocess.run(
                install_cmd,
                shell=True,
                capture_output=True,
                text=True,
                check=True
            )
            print("*****Ollama ì„¤ì¹˜ ì™„ë£Œ*****")
            return True
        except subprocess.CalledProcessError as e :
            print(f"Ollama ì„¤ì¹˜ ì‹¤íŒ¨ : {e}")
            print("https://ollama.com/download ë°©ë¬¸ í›„ Linux ë²„ì ¼ ì„¤ì¹˜ ìš”ë§")
            return False
    
    def start_ollama_server(self)->bool :
        ''' ollama ì„œë¹„ìŠ¤ ì‹œì‘ '''
        try :
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ Ollama ì„œë¹„ìŠ¤ ì‹¤í–‰
            subprocess.Popen(
                ['ollama','serve'],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
            )
            
            # ì„œë¹„ìŠ¤ê°€ ì‹œì‘í•  ë•Œ ê¹Œì§€ ëŒ€ê¸°
            for i in range(10) :
                try :
                    response = requests.get("http://localhost:11434/api/tags", timeout=2)
                    if response.status_code == 200 :
                        print("Ollama ì„œë¹„ìŠ¤ ì‹œì‘!!!!")
                        return True
                except requests.exceptions.RequestException as e :
                    print(f"Ollama ì„œë¹„ìŠ¤ ì‹œì‘ ëŒ€ê¸°ì¤‘ ({i+1}/10)...")
                    time.sleep(2)
            print("Ollama ì„œë¹„ìŠ¤ ì‹œì‘ ì‹¤íŒ¨")
            return False
        except Exception as e :
            print(f"Ollama ì„œë¹„ìŠ¤ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ : {e}")
            return False
    def select_optimal_model(self, available_vram: float) -> ModelConfig:
        """ì‚¬ìš© ê°€ëŠ¥í•œ VRAMì— ë”°ë¼ ìµœì  ëª¨ë¸ ì„ íƒ"""
        print(f"ğŸ¯ ì‚¬ìš© ê°€ëŠ¥í•œ VRAM: {available_vram:.1f}GB")
        
        # VRAMì— ë”°ë¥¸ ëª¨ë¸ ì„ íƒ
        if available_vram >= 7.5:
            model = self.available_models["qwen2.5:7b-instruct-q8_0"]
            print("âœ… Q8_0 (INT8) ì–‘ìí™” ëª¨ë¸ ì„ íƒ - ìµœê³  í’ˆì§ˆ")
        elif available_vram >= 4.5:
            model = self.available_models["qwen2.5:7b-instruct-q4_k_m"] 
            print("âœ… Q4_K_M (INT4) ì–‘ìí™” ëª¨ë¸ ì„ íƒ - ë©”ëª¨ë¦¬ íš¨ìœ¨")
        else:
            model = self.available_models["qwen2.5:7b-instruct-q4_k_m"]
            print("âš ï¸  VRAMì´ ë¶€ì¡±í•˜ì§€ë§Œ Q4_K_M ëª¨ë¸ë¡œ ì‹œë„í•©ë‹ˆë‹¤.")
        
        self.selected_model = model
        return model
    def download_model_with_ollama(self, model_config : ModelConfig) -> bool:
        """ Ollama ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        print(f"{model_config.name} ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘(size : {model_config.size}, quantization : {model_config.quantization})")
        print(f"ì˜ˆìƒ ë©”ëª¨ë¦¬ : {model_config.memory_requirement:.1f}GB")
        
        try :
            # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
            result = subprocess.run(
                ['ollama', 'pull', model_config.name],
                stdout = subprocess.PIPE,
                stderr = subprocess.PIPE,
                text=True,
            )
            
            # ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ì¶œë ¥
            while True :
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    print(output.strip())
            
            # process ëŒ€ê¸° ì™„ë£Œ
            return_code = process.wait()
            if return_code == 0:
                print(f"{model_config.name} ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                return True
            else:
                print(f"{model_config.name} ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                return False
        except Exception as e :
            print(f"Ollama ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ : {e}")
            return False
    def verify_model_downloaded(self,model_name :str) -> bool :
        """ ëª¨ë¸ ì •ìƒ ë‹¤ìš´ë¡œë“œ ì ê²€ """
        try :
            result = subprocess.run(
                ['ollama','list'],
                capture_output=True,
                text=True,
                check=True
            )
            # ë‹¤ìš´ë¡œë“œ ëœ ëª¨ë¸ ëª©ë¡ì—ì„œ í™•ì¸
            downloaded_models = result.stdout
            if model_name in downloaded_models :
                print(f"{model_name} ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                return True
            else :
                print(f"{model_name} ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                return False
        except Exception as e :
            print(f"ëª¨ë¸ í™•ì¸ ì¤‘ ì‹¤íŒ¨ì˜¤ë¥˜ : {e}")
            return False
    def install_vllm(self) -> bool :
        '''vLLM ì„¤ì¹˜'''
        print("vLLM ì„¤ì¹˜ ì‹œì‘")
        
        # CUDA ë²„ì ¼ì— ë§ëŠ” Pytorch ì„¤ì¹˜ í™•ì¸
        try :
            import torch
            if torch.cuda.is_available():
                print("CUDA ì •ìƒ")
            else :
                print("CUDA ì—ëŸ¬, CPU ëª¨ë“œ ì‹¤í–‰")
        except ImportError:
            print("torch ì„¤ì¹˜ ìš”ë§")
        
        # vllm ì„¤ì¹˜ ëª…ë ¹ì–´ë“¤
        install_commands = [
            # ê¸°ë³¸ vllm ì„¤ì¹˜
            [sys.executable, '-m', 'pip', 'install', 'vllm'],
            # ì¶”ê°€ ì˜ì¡´ì„± ì„¤ì¹˜
            [sys.executable, '-m', 'pip', 'install', 'transformers', 'accelerate'],
        ]
        
        for cmd in install_commands:
            try :
                print(f"ì‹¤í–‰ ì¤‘ : {' '.join(cmd)}")
                result = subprocess.run(cmd, check=True, capture_output=True, text=True)
                print("ì„¤ì¹˜ ì™„ë£Œ")
            except subprocess.CalledProcessError as e :
                print(f"ì„¤ì¹˜ ì‹¤íŒ¨ : {e}")
                print(f"ì˜¤ë¥˜ ì¶œë ¥ : {e.stderr}")
                return False
        
        # ì„¤ì¹˜ í™•ì¸
        try : 
            import vllm
            print("vLLM ì„¤ì¹˜ ì™„ë£Œ", vllm.__version__)
            return True
        except ImportError:
            print("vLLM ì„¤ì¹˜ ì‹¤íŒ¨")
            return False
    
# ëª¨ë“ˆ 2: vLLM ì„¤ì¹˜ ë° Ollama Qwen2.5:7b ì„¤ì •
# File: vllm_setup.py

import subprocess
import sys
import os
import json
import requests
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì • ì •ë³´ë¥¼ ë‹´ëŠ” ë°ì´í„°í´ë˜ìŠ¤"""
    name: str
    size: str
    quantization: str
    memory_requirement: float  # GB
    context_length: int
    download_url: Optional[str] = None

class VLLMSetup:
    """vLLMê³¼ Qwen2.5 ëª¨ë¸ ì„¤ì •ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, base_dir: str = "./models"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)
        
        # RTX 4060 8GB í™˜ê²½ì— ìµœì í™”ëœ ëª¨ë¸ ì„¤ì •
        self.available_models = {
            "qwen2.5:7b-instruct-q8_0": ModelConfig(
                name="qwen2.5:7b-instruct-q8_0",
                size="7B",
                quantization="Q8_0",  # INT8 ì–‘ìí™”
                memory_requirement=7.5,  # GB
                context_length=32768
            ),
            "qwen2.5:7b-instruct-q4_k_m": ModelConfig(
                name="qwen2.5:7b-instruct-q4_k_m", 
                size="7B",
                quantization="Q4_K_M",  # INT4 ì–‘ìí™” (ë” ì ˆì•½)
                memory_requirement=4.5,  # GB
                context_length=32768
            )
        }
        
        self.selected_model = None
        self.vllm_server_process = None
        
    def check_ollama_installed(self) -> Tuple[bool, Optional[str]]:
        """Ollama ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸"""
        try:
            result = subprocess.run(
                ["ollama", "--version"], 
                capture_output=True, 
                text=True, 
                check=True
            )
            version = result.stdout.strip()
            return True, version
        except (subprocess.CalledProcessError, FileNotFoundError):
            return False, None
    
    def install_ollama(self) -> bool:
        """Ollama ì„¤ì¹˜ (Ubuntu/Linux í™˜ê²½)"""
        print("ğŸš€ Ollama ì„¤ì¹˜ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        try:
            # Ollama ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
            install_cmd = "curl -fsSL https://ollama.com/install.sh | sh"
            result = subprocess.run(
                install_cmd,
                shell=True,
                check=True,
                text=True,
                capture_output=True
            )
            
            print("âœ… Ollama ì„¤ì¹˜ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            return True
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ Ollama ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
            print("ìˆ˜ë™ ì„¤ì¹˜ ê°€ì´ë“œ:")
            print("1. https://ollama.com/download ë°©ë¬¸")
            print("2. Linux ë²„ì „ ë‹¤ìš´ë¡œë“œ í›„ ì„¤ì¹˜")
            return False
    
    def start_ollama_service(self) -> bool:
        """Ollama ì„œë¹„ìŠ¤ ì‹œì‘"""
        try:
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ Ollama ì„œë¹„ìŠ¤ ì‹œì‘
            subprocess.Popen(
                ["ollama", "serve"],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL
            )
            
            # ì„œë¹„ìŠ¤ê°€ ì‹œì‘ë  ë•Œê¹Œì§€ ëŒ€ê¸°
            for i in range(10):
                try:
                    response = requests.get("http://localhost:11434/api/tags", timeout=2)
                    if response.status_code == 200:
                        print("âœ… Ollama ì„œë¹„ìŠ¤ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        return True
                except requests.exceptions.RequestException:
                    print(f"â³ Ollama ì„œë¹„ìŠ¤ ì‹œì‘ ëŒ€ê¸° ì¤‘... ({i+1}/10)")
                    time.sleep(2)
            
            print("âŒ Ollama ì„œë¹„ìŠ¤ ì‹œì‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return False
            
        except Exception as e:
            print(f"âŒ Ollama ì„œë¹„ìŠ¤ ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def select_optimal_model(self, available_vram: float) -> ModelConfig:
        """ì‚¬ìš© ê°€ëŠ¥í•œ VRAMì— ë”°ë¼ ìµœì  ëª¨ë¸ ì„ íƒ"""
        print(f"ğŸ¯ ì‚¬ìš© ê°€ëŠ¥í•œ VRAM: {available_vram:.1f}GB")
        
        # VRAMì— ë”°ë¥¸ ëª¨ë¸ ì„ íƒ
        if available_vram >= 7.5:
            model = self.available_models["qwen2.5:7b-instruct-q8_0"]
            print("âœ… Q8_0 (INT8) ì–‘ìí™” ëª¨ë¸ ì„ íƒ - ìµœê³  í’ˆì§ˆ")
        elif available_vram >= 4.5:
            model = self.available_models["qwen2.5:7b-instruct-q4_k_m"] 
            print("âœ… Q4_K_M (INT4) ì–‘ìí™” ëª¨ë¸ ì„ íƒ - ë©”ëª¨ë¦¬ íš¨ìœ¨")
        else:
            model = self.available_models["qwen2.5:7b-instruct-q4_k_m"]
            print("âš ï¸  VRAMì´ ë¶€ì¡±í•˜ì§€ë§Œ Q4_K_M ëª¨ë¸ë¡œ ì‹œë„í•©ë‹ˆë‹¤.")
        
        self.selected_model = model
        return model
    
    def download_model_with_ollama(self, model_config: ModelConfig) -> bool:
        """Ollamaë¥¼ í†µí•´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        print(f"ğŸ“¥ {model_config.name} ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...")
        print(f"   í¬ê¸°: {model_config.size}, ì–‘ìí™”: {model_config.quantization}")
        print(f"   ì˜ˆìƒ ë©”ëª¨ë¦¬: {model_config.memory_requirement}GB")
        
        try:
            # Ollama pull ëª…ë ¹ì–´ ì‹¤í–‰
            process = subprocess.Popen(
                ["ollama", "pull", model_config.name],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            # ì‹¤ì‹œê°„ ì§„í–‰ìƒí™© ì¶œë ¥
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    print(f"   {output.strip()}")
            
            # í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ëŒ€ê¸°
            return_code = process.wait()
            
            if return_code == 0:
                print("âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                return True
            else:
                error_output = process.stderr.read()
                print(f"âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {error_output}")
                return False
                
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def verify_model_download(self, model_name: str) -> bool:
        """ëª¨ë¸ì´ ì œëŒ€ë¡œ ë‹¤ìš´ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        try:
            result = subprocess.run(
                ["ollama", "list"],
                capture_output=True,
                text=True,
                check=True
            )
            
            # ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ ëª©ë¡ì—ì„œ í™•ì¸
            downloaded_models = result.stdout
            if model_name in downloaded_models:
                print(f"âœ… {model_name} ëª¨ë¸ í™•ì¸ ì™„ë£Œ!")
                return True
            else:
                print(f"âŒ {model_name} ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
                
        except subprocess.CalledProcessError as e:
            print(f"âŒ ëª¨ë¸ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def install_vllm(self) -> bool:
        """vLLM ì„¤ì¹˜"""
        print("ğŸš€ vLLM ì„¤ì¹˜ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # CUDA ë²„ì „ì— ë§ëŠ” PyTorch ì„¤ì¹˜ í™•ì¸
        try:
            import torch
            if torch.cuda.is_available():
                print(f"âœ… CUDA PyTorch ê°ì§€: {torch.version.cuda}")
            else:
                print("âš ï¸  CUDA PyTorchê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        except ImportError:
            print("âŒ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            return False
        
        # vLLM ì„¤ì¹˜ ëª…ë ¹ì–´ë“¤
        install_commands = [
            # ê¸°ë³¸ vLLM ì„¤ì¹˜
            [sys.executable, "-m", "pip", "install", "vllm"],
            # ì¶”ê°€ ì˜ì¡´ì„± ì„¤ì¹˜  
            [sys.executable, "-m", "pip", "install", "transformers", "accelerate"]
        ]
        
        for cmd in install_commands:
            try:
                print(f"ì‹¤í–‰ ì¤‘: {' '.join(cmd)}")
                result = subprocess.run(cmd, check=True, capture_output=True, text=True)
                print("âœ… ì„¤ì¹˜ ì™„ë£Œ!")
            except subprocess.CalledProcessError as e:
                print(f"âŒ ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
                print(f"ì˜¤ë¥˜ ì¶œë ¥: {e.stderr}")
                return False
        
        # ì„¤ì¹˜ í™•ì¸
        try:
            import vllm
            print(f"âœ… vLLM ì„¤ì¹˜ í™•ì¸ ì™„ë£Œ! ë²„ì „: {vllm.__version__}")
            return True
        except ImportError:
            print("âŒ vLLM ì„¤ì¹˜ í™•ì¸ ì‹¤íŒ¨!")
            return False
    
    def create_vllm_config(self, model_config: ModelConfig) -> Dict:
        """vLLM ì„œë²„ ì„¤ì • ìƒì„±"""
        config = {
            "model": model_config.name,
            "host": "0.0.0.0",
            "port": 8000,
            "gpu-memory-utilization": 0.8,  # GPU ë©”ëª¨ë¦¬ì˜ 80% ì‚¬ìš©
            "max-model-len": min(model_config.context_length, 8192),  # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
            "trust-remote-code": True,
            "enforce-eager": True,  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ ì„¤ì •
        }
        
        # RTX 4060 ìµœì í™” ì„¤ì •
        if model_config.quantization.startswith("Q4"):
            config.update({
                "quantization": "awq",  # 4ë¹„íŠ¸ ì–‘ìí™” í™œìš©
                "max-num-batched-tokens": 2048,  # ë°°ì¹˜ í¬ê¸° ì œí•œ
            })
        
        return config
    
    def save_config(self, config: Dict, filename: str = "vllm_config.json"):
        """ì„¤ì •ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
        config_path = self.base_dir / filename
        with open(config_path, "w", encoding="utf-8") as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        print(f"ì„¤ì • íŒŒì¼ ì €ì¥: {config_path}")
        return config_path
    
    def setup_complete_environment(self, available_vram: float = 8.0) -> bool:
        """ì „ì²´ í™˜ê²½ ì„¤ì •ì„ ìë™ìœ¼ë¡œ ìˆ˜í–‰"""
        print("vLLM + Qwen2.5 í™˜ê²½ ì„¤ì • ì‹œì‘!")
        print("=" * 60)
        
        # 1. Ollama í™•ì¸ ë° ì„¤ì¹˜
        print("Ollama í™•ì¸")
        ollama_installed, version = self.check_ollama_installed()
        
        if not ollama_installed:
            if not self.install_ollama():
                return False
        else:
            print(f"Ollama ì´ë¯¸ ì„¤ì¹˜ë¨: {version}")
        
        # 2. Ollama ì„œë¹„ìŠ¤ ì‹œì‘
        print("\n Ollama ì„œë¹„ìŠ¤ ì‹œì‘ ì¤‘...")
        if not self.start_ollama_service():
            return False
        
        # 3. ìµœì  ëª¨ë¸ ì„ íƒ
        print("\n ìµœì  ëª¨ë¸ ì„ íƒ ì¤‘...")
        model_config = self.select_optimal_model(available_vram)
        
        # 4. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        print("\n ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        if not self.download_model_with_ollama(model_config):
            return False
        
        # 5. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸
        print("\n ëª¨ë¸ í™•ì¸ ì¤‘...")
        if not self.verify_model_download(model_config.name):
            return False
        
        # 6. vLLM ì„¤ì¹˜
        print("\n vLLM ì„¤ì¹˜ ì¤‘...")
        if not self.install_vllm():
            return False
        
        # 7. ì„¤ì • íŒŒì¼ ìƒì„±
        print("\n ì„¤ì • íŒŒì¼ ìƒì„± ì¤‘...")
        vllm_config = self.create_vllm_config(model_config)
        self.save_config(vllm_config)
        
        print("\n ëª¨ë“  ì„¤ì • ì™„ë£Œ")
        print(f"ì„ íƒëœ ëª¨ë¸: {model_config.name}")
        print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {model_config.memory_requirement}GB")
        print("ë‹¤ìŒ ë‹¨ê³„ì—ì„œ vLLM ì„œë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        return True

if __name__ == "__main__":
    # RTX 4060 8GB í™˜ê²½ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
    setup = VLLMSetup()
    
    # í™˜ê²½ ì„¤ì • ì‹¤í–‰
    success = setup.setup_complete_environment(available_vram=8.0)
    
    if success:
        print("\nğŸš€ ì„¤ì • ì™„ë£Œ! ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ vLLM ì„œë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
        print("python -m vllm.entrypoints.openai.api_server --config vllm_config.json")
    else:
        print("\nâŒ ì„¤ì • ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")