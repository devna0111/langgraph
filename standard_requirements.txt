# vLLM 및 관련 라이브러리(torch는 cuda 버젼 체크하면서 설치 필요)
vllm>=0.2.7
torch>=2.0.0
transformers>=4.35.0
tokenizers>=0.14.0

# LangGraph 및 LangChain
langgraph>=0.0.30
langchain>=0.1.0
langchain-community>=0.0.10

# 유틸리티
requests>=2.31.0
aiohttp>=3.9.0
pydantic>=2.5.0
python-dotenv>=1.0.0

# 개발 및 테스트
pytest>=7.4.0
pytest-asyncio>=0.21.0
black>=23.0.0
flake8>=6.1.0

# 모니터링
psutil>=5.9.0
nvidia-ml-py>=12.535.0